{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d265461",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "The Markov Decision Process is a precursor concept to the Reinforcement Learning. Basically, A Markov Decision Process, also known as an MDP model, contains the following set of features:\n",
    "<ul> \n",
    "    <li>A set of possible states, S.</li>\n",
    "    <li>A set of models. </li>\n",
    "    <li>A set of possible actions, A. </li>\n",
    "    <li>A real-valued reward function, R(s, a). </li>\n",
    "    <li>A solution to the Markov Decision Process. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4280b",
   "metadata": {},
   "source": [
    "## Frozen Lake Problem\n",
    "The Frozen Lake problem is a classic grid-world problem used in reinforcement learning to\n",
    "demonstrate and test various algorithms. It's a simple but illustrative problem that involves an agent\n",
    "navigating a grid while facing challenges. Do not find the optimal poolicy, just keep track of maximum reward (minimum 'F' states).\n",
    "\n",
    "### Environment\n",
    "<ul>\n",
    "    <li>The environment is represented as a grid, typically a 4x4 or 8x8 grid. </li>\n",
    "    <li>The grid consists of different types of cells:\n",
    "        <ul>\n",
    "            <li>\"S\" (Start): The starting point for the agent. </li>\n",
    "            <li>\"F\" (Frozen): Safe frozen surface, which the agent can walk on without any issue. </li>\n",
    "            <li>\"H\" (Hole): Holes in the frozen surface. If the agent steps into a hole, it falls and fails. </li>\n",
    "            <li>\"G\" (Goal): The goal location the agent needs to reach. </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "### Agent\n",
    "<ul>\n",
    "    <li>The agent starts at the \"S\" cell and needs to navigate through the grid to reach the \"G\" cell.</li>\n",
    "    <li>The agent can take discrete actions such as moving UP, DOWN, LEFT, or RIGHT.</li>\n",
    "</ul>\n",
    "\n",
    "### Objective\n",
    "The goal of the agent is to reach the \"G\" cell while avoiding the \"H\" cells. Success is defined\n",
    "as reaching the goal cell.\n",
    "\n",
    "### Challenges\n",
    "<ul>\n",
    "    <li>The ice on the frozen surface is slippery, so the agent doesn't always move in the intended\n",
    "        direction. Instead, it moves in the chosen direction with a certain probability, often making it\n",
    "        challenging to reach the goal.\n",
    "    </li>\n",
    "    <li>The agent's objective is to learn a policy that maximizes the cumulative reward while\n",
    "        navigating the grid. \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "### Rewards\n",
    "The rewards are given out in the following manner:\n",
    "<ul>\n",
    "    <li>Reaching the goal (\"G\") cell: +1 (positive reward for success)</li>\n",
    "    <li>Falling into a hole (\"H\") cell: -1 (negative reward for failure) </li>\n",
    "    <li> All other actions: -0.1 (a small negative reward for taking actions, which encourages the\n",
    "    agent to reach the goal with fewer steps) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa310d",
   "metadata": {},
   "source": [
    "# Code Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9c7c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class FrozenMDP:\n",
    "    def __init__(self, nc, nr):\n",
    "        self.nc = nc\n",
    "        self.nr = nr\n",
    "\n",
    "    def states(self):\n",
    "        return np.arange(1, self.nc * self.nr + 1)\n",
    "\n",
    "    def get_block_no(self, s):\n",
    "        return (s - 1) // self.nc + 1, (s - 1) % self.nc + 1\n",
    "\n",
    "    def get_state_no(self, x, y):\n",
    "        return (x - 1) * self.nc + y\n",
    "\n",
    "    def Start(self):\n",
    "        return 1\n",
    "\n",
    "    def Goal(self):\n",
    "        return self.nc * self.nr\n",
    "\n",
    "    def actions(self, s):\n",
    "        action = []\n",
    "        x, y = self.get_block_no(s)\n",
    "        if y - 1 >= 1:\n",
    "            action.append('Left')\n",
    "        if y + 1 <= self.nc:\n",
    "            action.append('Right')\n",
    "        if x - 1 >= 1:\n",
    "            action.append('Up')\n",
    "        if x <= self.nr:\n",
    "            action.append('Down')\n",
    "\n",
    "        return action\n",
    "\n",
    "    def FailureStates(self):\n",
    "        return [6, 8, 12, 13]\n",
    "\n",
    "    def SuccessState(self):\n",
    "        return [16]\n",
    "\n",
    "    def isGoal(self, s):\n",
    "        if s in self.FailureStates():\n",
    "            return True\n",
    "        if s in self.SuccessState():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def reward(self, state, action, new_state):\n",
    "        if new_state in self.states():\n",
    "            if new_state in self.FailureStates():\n",
    "                return -1\n",
    "            elif self.isGoal(new_state):\n",
    "                return 1\n",
    "\n",
    "            return -0.1\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def get_valid_neighbors(self, state):\n",
    "        neighbors = set()\n",
    "        x, y = self.get_block_no(state)\n",
    "\n",
    "        def is_valid_neighbor(x_neighbor, y_neighbor):\n",
    "            return (\n",
    "                0 <= x_neighbor < self.nr\n",
    "                and 0 <= y_neighbor < self.nc\n",
    "                and self.get_state_no(x_neighbor, y_neighbor) in self.FailureStates()\n",
    "            )\n",
    "\n",
    "        # Check up\n",
    "        if is_valid_neighbor(x, y - 1):\n",
    "            neighbors.add(self.get_state_no(x, y - 1))\n",
    "\n",
    "        # Check down\n",
    "        if is_valid_neighbor(x, y + 1):\n",
    "            neighbors.add(self.get_state_no(x, y + 1))\n",
    "\n",
    "        # Check left\n",
    "        if is_valid_neighbor(x - 1, y):\n",
    "            neighbors.add(self.get_state_no(x - 1, y))\n",
    "\n",
    "        # Check right\n",
    "        if is_valid_neighbor(x + 1, y):\n",
    "            neighbors.add(self.get_state_no(x + 1, y))\n",
    "\n",
    "        return sorted(neighbors)\n",
    "\n",
    "    def transition_probability(self, s, a, new_state):\n",
    "        x, y = self.get_block_no(s)\n",
    "\n",
    "        if a == \"Left\":\n",
    "            y -= 1\n",
    "        elif a == \"Right\":\n",
    "            y += 1\n",
    "        elif a == \"Up\":\n",
    "            x -= 1\n",
    "        elif a == \"Down\":\n",
    "            x += 1\n",
    "\n",
    "        s_calc = self.get_state_no(x, y)\n",
    "\n",
    "        if s_calc == new_state:\n",
    "            return 0.6\n",
    "        elif new_state in self.FailureStates():\n",
    "            return 0.4\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def transition(self, s, a):\n",
    "        x, y = self.get_block_no(s)\n",
    "\n",
    "        if a == \"Left\":\n",
    "            y -= 1\n",
    "        elif a == \"Right\":\n",
    "            y += 1\n",
    "        elif a == \"Up\":\n",
    "            x -= 1\n",
    "        elif a == \"Down\":\n",
    "            x += 1\n",
    "\n",
    "        s_calc = self.get_state_no(x, y)\n",
    "        return s_calc\n",
    "\n",
    "    def transitions(self, s):\n",
    "        transition_states = []\n",
    "        actions = self.actions(s)\n",
    "        for action in actions:\n",
    "            transition_states.append(self.transition(s, action))\n",
    "\n",
    "        return transition_states\n",
    "\n",
    "\n",
    "def minimum_reward_to_goal(mdp, state, total_reward, visited, parent_map):\n",
    "    # Base case: if state is goal, return the path and the total reward\n",
    "    if mdp.isGoal(state):\n",
    "        path = [state]\n",
    "        current_state = state\n",
    "        while current_state != mdp.Start():\n",
    "            if parent_map[current_state] <= (mdp.nc * mdp.nr):\n",
    "                path.insert(0, parent_map[current_state])\n",
    "            current_state = parent_map[current_state]\n",
    "        return total_reward, state, path,  'Failure' if state in mdp.FailureStates() else 'Goal'\n",
    "\n",
    "    # Mark the current state as visited\n",
    "    visited.add(state)\n",
    "\n",
    "    # Initialize the minimum reward and the best path to None\n",
    "    min_reward = None\n",
    "    best_path = None\n",
    "\n",
    "    for action in mdp.actions(state):\n",
    "        next_state = mdp.transition(state, action)\n",
    "        reward = mdp.reward(state, action, next_state)\n",
    "\n",
    "        if next_state not in visited and next_state not in mdp.FailureStates():\n",
    "            parent_map[next_state] = state\n",
    "\n",
    "            # Recursively find the minimum reward and the best path from the next state\n",
    "            sub_reward, sub_goal, sub_path, sub_status = minimum_reward_to_goal(\n",
    "                mdp, next_state, total_reward + reward, visited, parent_map)\n",
    "\n",
    "            # If the sub_reward is not None and it is smaller than the current min_reward, update the min_reward and the best_path\n",
    "            if sub_reward is not None and (min_reward is None or sub_reward < min_reward):\n",
    "                min_reward = sub_reward\n",
    "                best_path = sub_path\n",
    "\n",
    "    # Return the minimum reward and the best path\n",
    "    return min_reward,None, best_path, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0b70f",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee78ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: 1\n",
      "Reward: 0.5 having path [1, 2, 3, 7, 11, 10, 14, 15, 16] \n"
     ]
    }
   ],
   "source": [
    "a = FrozenMDP(4, 4)\n",
    "print(\"Initial State:\", a.Start())\n",
    "r, n, p, s = minimum_reward_to_goal(mdp=a, state=a.Start(), total_reward=0, visited=set(), parent_map={})\n",
    "r += 0.20\n",
    "print(f\"Reward: {r} having path {p} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
