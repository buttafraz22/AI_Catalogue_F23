{
 "cells": [
  {
   "attachments": {
    "bridge-cross.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "91fc83aa",
   "metadata": {},
   "source": [
    "# Bridge Crossing\n",
    "\n",
    "The bridge crossing model is another example of a Markov Decision Process. What you are concerned with in this scenario is the route. We are not saying it is the <em>optimal</em> one, but slowly, it will teach our intelligent agent about policy decisions.\n",
    "\n",
    "Recall that a policy in a Markov Decision Process (MDP) is a rule that tells the agent what action to take in each state. A policy can be deterministic or probabilistic.\n",
    "\n",
    "For Reference, this is the <em>approximation</em> of the model we will develop. We will keep its abstraction intact.\n",
    "![bridge-cross.png](attachment:bridge-cross.png)\n",
    "\n",
    "In this model (<em>which is a matrix representing the world</em>), the 2 cost is the reward for hitting the bottom node. If we hit the red nodes, its -50 in reward.\n",
    "We start from the node (1,1) which is basically the top left node. For this instance of the problem, the goal node is (1,6) which is the top right node.\n",
    "\n",
    "<ul>\n",
    "    <li>The agent has a choice(validations are there ofcourse) to move in four directions, North, East, West, South.</li>\n",
    "    <li>The probability of each direction is 0.25 .</li>\n",
    "    <li>The cost of each node is constant = -0.1 .</li>\n",
    "    <li>The agent is expected to take a random path.</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Note:</strong><em>There is a possible slip factor having probability 0.8 in this case for the squares immediately next to the red squares. You are supposed to terminate the agent's search on hitting that probability. Also, the $\\gamma$\n",
    "factor for this problem is 1.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074d160",
   "metadata": {},
   "source": [
    "# Node Class\n",
    "The node class represents the step of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfd9efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgeNode:\n",
    "    def __init__(self, loc, slip_probability=0, cost=-0.1):\n",
    "        \"\"\"\n",
    "            Represents the step of the agent.\n",
    "            \n",
    "            Args:\n",
    "                loc (Tuple) : (x,y) representing the location of the node. Start has 1,1.\n",
    "                slip_probability (float) : Representing the slip probability of the node. Default is 0.\n",
    "                cost(float) : The cost of the node. Default is -0.1 .\n",
    "        \"\"\"\n",
    "        self.loc = loc\n",
    "        self.slip_probability = slip_probability\n",
    "        self.cost = cost\n",
    "    \n",
    "    def get_adjacent_node(self, lim_X, lim_Y):\n",
    "        \"\"\"\n",
    "            Get co-ordinates of nodes adjacent to self.\n",
    "            \n",
    "            Args:\n",
    "                lim_X (int) : The rows of the puzzle.\n",
    "                lim_Y (int) : The columns of the puzzle.\n",
    "            \n",
    "            Returns:\n",
    "                List : A list of nodes adjacent to self nodes. Does not include the node having slip-probability 0.\n",
    "        \"\"\"\n",
    "        adjacent_nodes = []\n",
    "        if self.loc[0] >= 1:\n",
    "            adjacent_nodes.append((self.loc[0] - 1, self.loc[1])) # left\n",
    "        if self.loc[0] < lim_X:\n",
    "            adjacent_nodes.append((self.loc[0] + 1, self.loc[1])) # right\n",
    "        if self.loc[1] > 1:\n",
    "            adjacent_nodes.append((self.loc[0], self.loc[1] - 1)) # up\n",
    "        if self.loc[1] < lim_Y:\n",
    "            adjacent_nodes.append((self.loc[0], self.loc[1] + 1)) # down\n",
    "\n",
    "        # Filter out nodes that have a slip probability of 0.\n",
    "        adjacent_nodes = [node for node in adjacent_nodes]\n",
    "\n",
    "        return adjacent_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caddb62",
   "metadata": {},
   "source": [
    "# Bridge Crossing Problem\n",
    "The Bridge Crossing Problem models the solution of the problem according to the criteria given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "465b4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class BridgeCrossing:\n",
    "    \n",
    "    def __init__(self, list_red_sq, rows=4, cols=6, end_loc=(1,6)):\n",
    "        \"\"\"\n",
    "        Models the solution of the problem according to the criteria given.\n",
    "        \n",
    "        Args:\n",
    "            rows(int): The number of rows in the BridgeCrossing problem. Default value is 4.\n",
    "            cols(int): The number of columns in the BridgeCrossing problem. Default = 6.\n",
    "            end_loc (tuple): The goal state of the model. Default=(1,6)\n",
    "            list_red_sq [List(tuples)] : The list of squares for which we want to set the cost as -50. Indexed from 1.\n",
    "        \"\"\"\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.end_loc = end_loc\n",
    "        self.list_red_sq = list_red_sq\n",
    "        self.world = []\n",
    "    \n",
    "    def build_world(self):\n",
    "        self.world = [[BridgeNode((i+1, j+1)) for j in range(self.cols)] for i in range(self.rows)]\n",
    "\n",
    "        # Update the cost of the red path\n",
    "        for x in self.world:\n",
    "            for y in x:\n",
    "                if y.loc in self.list_red_sq:\n",
    "                    y.cost = -50\n",
    "        \n",
    "        # Update the slip probabilities\n",
    "        for node in self.list_red_sq:\n",
    "            adj_nodes = self.world[node[0] - 1][node[1] - 1].get_adjacent_node(self.rows, self.cols)\n",
    "            for slip_node in adj_nodes:\n",
    "                n = self.world[slip_node[0] - 1][slip_node[1] - 1]\n",
    "                if n.cost != -50 and n.loc[0] <= slip_node[0]: # This AND validation part 2 is troublesome. Basically,\n",
    "                                                               # (0,5) node (0-based indexing) is returning (4,5) as adjacent node\n",
    "                                                               # for God knows what reason. So as a stop-gap measure, this AND\n",
    "                                                               # validation is applied.\n",
    "                    n.slip_probability = 0.8\n",
    "        \n",
    "        # Update the cost of the goal to +500\n",
    "        self.world[self.end_loc[0] - 1][self.end_loc[1] - 1].cost = 500\n",
    "            \n",
    "        \n",
    "    def print_world(self):\n",
    "        \"\"\"\n",
    "        Utility function built for debugging purposes. Has no say in the working of the model.\n",
    "        \"\"\"\n",
    "        for row in self.world:\n",
    "            for node in row:\n",
    "                print(f\"{node.loc}: {node.cost} ,{node.slip_probability}\")\n",
    "    \n",
    "    def start_state(self):\n",
    "        \"\"\"\n",
    "        Return the start state of the model.\n",
    "        Returns:\n",
    "            Tuple(loc) : 1 based index that returns the start state(1,1).\n",
    "        \"\"\"\n",
    "        return (1,1)\n",
    "    \n",
    "    def transition_probability(self, state, action, state_new):\n",
    "        \"\"\"\n",
    "        Return the transition probability from s to s_new.\n",
    "        For this problem, the probability to move in all four directions is the same. Except the edge cases.\n",
    "        So the default return value is 0.25 for all directions.\n",
    "        \n",
    "        Args:\n",
    "            state (Tuple): state that is the location for actions\n",
    "            action: action:\n",
    "            state_new(Tuple): The new state location.\n",
    "        \n",
    "        Returns:\n",
    "            float: The value of the probability corresponding to that action.\n",
    "        \"\"\"\n",
    "        probability = 0\n",
    "        is_valid_move = True\n",
    "        \n",
    "        # Check if the action is valid in the first place\n",
    "        \n",
    "        if action == 'left' and state[0] < 1:\n",
    "            probability = 0\n",
    "            is_valid_move = False\n",
    "        \n",
    "        if action == 'right' and state[0] > self.cols:\n",
    "            probability = 0\n",
    "            is_valid_move = False\n",
    "        \n",
    "        if action == 'down' and state[1] > self.rows:\n",
    "            probability = 0\n",
    "            is_valid_move = False\n",
    "        \n",
    "        if action == 'up' and state[1] < 1:\n",
    "            probability = 0\n",
    "            is_valid_move = False\n",
    "        \n",
    "        # All validations have passed\n",
    "        if is_valid_move:\n",
    "            probability = 1   # This move can be made in that direction provided\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        Return the next state corresponding to the action a.\n",
    "        Args:\n",
    "            s (tuple[int]) :The state of the agent where it is standing.\n",
    "            a (str) : The action to take.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple(loc) : 1 based index that returns the start state(1,1).\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "#         print(state)\n",
    "        if action == 'left' and state[1] > 1:\n",
    "            y -= 1\n",
    "        \n",
    "        if action == 'right' and state[1] < self.cols:\n",
    "            y += 1\n",
    "        \n",
    "        if action == 'down' and state[0] < self.rows:\n",
    "            x += 1\n",
    "        \n",
    "        if action == 'up' and state[0] >  1:\n",
    "            x -= 1\n",
    "        \n",
    "#         print(x,y)\n",
    "        return (x,y)\n",
    "    \n",
    "    def transition(self, s, a):\n",
    "        \"\"\"\n",
    "        Make the transition from the state s using the action a.\n",
    "        Args:\n",
    "            s (tuple[int]) :The state of the agent where it is standing.\n",
    "            a (str) : The action to take.\n",
    "        \"\"\"\n",
    "        \n",
    "        new_loc = self.get_next_state(s, a)\n",
    "        \n",
    "        tran_prob = self.transition_probability(s, a, new_loc)\n",
    "        if tran_prob == 1 and new_loc != s:\n",
    "            return new_loc\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def process(self, discount_factor=1):\n",
    "        \"\"\"\n",
    "        Find the route of the agent from the start state till the goal state. If the slip probability hits on the slip node,\n",
    "        The agent should terminate its search.\n",
    "        \"\"\"\n",
    "        policy = []\n",
    "        is_goal_found = False\n",
    "        s = self.start_state()\n",
    "        policy.append(s)\n",
    "\n",
    "        utility = 0\n",
    "\n",
    "        while not is_goal_found:\n",
    "            up_tran = self.transition(s, 'up')\n",
    "            down_tran = self.transition(s, 'down')\n",
    "            left_tran = self.transition(s, 'left')\n",
    "            right_tran = self.transition(s, 'right')\n",
    "\n",
    "            # First, filter out the None transitions and the transitions to nodes with cost -50.\n",
    "            agent_transitions = []\n",
    "            for tran in [up_tran, down_tran, left_tran, right_tran]:\n",
    "                if tran is not None and self.world[tran[0] - 1][tran[1] - 1].cost != -50:\n",
    "                    agent_transitions.append((tran[0], tran[1]))\n",
    "\n",
    "            # If there are no valid transitions, then the agent has reached a dead end and should terminate its search.\n",
    "            if len(agent_transitions) == 0:\n",
    "                break\n",
    "\n",
    "            # Otherwise, randomly choose one of the valid transitions and update the agent's state.\n",
    "            if agent_transitions:\n",
    "                # Check if the current state is a slip node.\n",
    "                if self.world[s[0] - 1][s[1] - 1].slip_probability > 0:\n",
    "                    # Generate a random number between 0 and 1.\n",
    "                    random_num = random.random()\n",
    "\n",
    "                    # If the random number is greater than or equal to the slip probability, then terminate the search.\n",
    "                    if random_num >= self.world[s[0] - 1][s[1] - 1].slip_probability:\n",
    "                        break\n",
    "\n",
    "                next_state = random.choice(agent_transitions)\n",
    "                s = tuple(next_state)\n",
    "                policy.append(s)\n",
    "\n",
    "                if s == self.end_loc:\n",
    "                    is_goal_found = True\n",
    "\n",
    "                # Calculate the utility of the current state, taking into account the discount factor.\n",
    "                utility = utility + discount_factor * self.world[s[0] - 1][s[1] - 1].cost\n",
    "\n",
    "        return policy, utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066e60b",
   "metadata": {},
   "source": [
    "# Driver Code\n",
    "\n",
    "Run the search to (1,6). If the search is run correctly, the utility should be greater than 450. Otherwise a negative answer indicates a dead end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b08a003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.19999999999999\n",
      "[(1, 1), (1, 2), (1, 1), (2, 1), (1, 1), (2, 1), (1, 1), (1, 2), (1, 1), (2, 1), (1, 1), (2, 1), (3, 1), (4, 1), (4, 2), (3, 2), (3, 1), (3, 2), (3, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 1), (1, 1), (1, 2), (2, 2), (2, 1), (2, 2), (3, 2), (3, 3), (3, 2), (3, 1), (2, 1), (3, 1), (2, 1), (1, 1), (1, 2), (1, 1), (2, 1), (3, 1), (4, 1), (3, 1), (3, 2), (2, 2), (2, 3), (3, 3), (3, 2), (3, 3), (4, 3), (3, 3), (3, 2), (4, 2), (4, 1), (3, 1), (4, 1), (3, 1), (4, 1), (3, 1), (4, 1), (3, 1), (3, 2), (3, 1), (4, 1), (4, 2), (3, 2), (3, 1), (2, 1), (1, 1), (1, 2), (1, 3), (1, 4), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "world = BridgeCrossing(list_red_sq=[(1,5), (2,5)])\n",
    "world.build_world()\n",
    "# world.print_world()\n",
    "# print(world.transition((2,2), 'left'))\n",
    "\n",
    "policy, utility = world.process()\n",
    "print(utility)\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
